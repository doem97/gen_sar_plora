{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Inference on CE conditions](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Inference on CE conditions](#toc1_)    \n",
    "- [Inference on CAM map](#toc2_)    \n",
    "    - [0: Single Image Inference](#toc2_1_1_)    \n",
    "    - [1: Single Image Inference](#toc2_1_2_)    \n",
    "    - [1: Batch Inference](#toc2_1_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/control/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model config from [./models/cldm_v15.yaml]\n",
      "Loaded state_dict from [./checkpoints/fusar_v1/fusar_v1_ce-epoch=9.ckpt]\n"
     ]
    }
   ],
   "source": [
    "from share import *\n",
    "import config\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "from annotator.canny import CannyDetector\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "\n",
    "\n",
    "apply_canny = CannyDetector()\n",
    "\n",
    "model = create_model(\"./models/cldm_v15.yaml\").cpu()\n",
    "# model.load_state_dict(load_state_dict('./models/control_sd15_canny.pth', location='cuda'))\n",
    "model.load_state_dict(\n",
    "    load_state_dict(\"./checkpoints/fusar_v1/fusar_v1_ce-epoch=9.ckpt\", location=\"cuda\")\n",
    ")\n",
    "\n",
    "model = model.cuda()\n",
    "ddim_sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotate_image(image, random_angle=0):\n",
    "    rows, cols, _ = image.shape\n",
    "    # random_angle = np.random.uniform(min_angle, max_angle)\n",
    "    random_angle = random_angle\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((cols / 2, rows / 2), random_angle, 1)\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (cols, rows))\n",
    "    return rotated_image\n",
    "\n",
    "\n",
    "def process(\n",
    "    input_image,\n",
    "    prompt,\n",
    "    output_dir=\"./gen/debug\",\n",
    "    a_prompt=\"SAR image\",\n",
    "    n_prompt=\"colored image\",\n",
    "    guess_mode=False,\n",
    "    num_samples=1,\n",
    "    image_resolution=512,\n",
    "    ddim_steps=20,\n",
    "    strength=1.0,\n",
    "    scale=9.0,\n",
    "    seed=12345,\n",
    "    eta=0.0,\n",
    "    low_threshold=100,\n",
    "    high_threshold=200,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        img_name = os.path.basename(input_image)\n",
    "        input_image = cv2.imread(input_image)\n",
    "        img = resize_image(HWC3(input_image), image_resolution)\n",
    "        # img = random_rotate_image(img, random_angle=60)\n",
    "        H, W, C = img.shape\n",
    "\n",
    "        detected_map = apply_canny(img, low_threshold, high_threshold)\n",
    "        detected_map = HWC3(detected_map)\n",
    "\n",
    "        control = torch.from_numpy(detected_map.copy()).float().cuda() / 255.0\n",
    "        control = torch.stack([control for _ in range(num_samples)], dim=0)\n",
    "        control = einops.rearrange(control, \"b h w c -> b c h w\").clone()\n",
    "\n",
    "        if seed == -1:\n",
    "            seed = random.randint(0, 65535)\n",
    "        seed_everything(seed)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        cond = {\n",
    "            \"c_concat\": [control],\n",
    "            \"c_crossattn\": [\n",
    "                model.get_learned_conditioning([prompt + \", \" + a_prompt] * num_samples)\n",
    "            ],\n",
    "        }\n",
    "        un_cond = {\n",
    "            \"c_concat\": None if guess_mode else [control],\n",
    "            \"c_crossattn\": [model.get_learned_conditioning([n_prompt] * num_samples)],\n",
    "        }\n",
    "        shape = (4, H // 8, W // 8)\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=True)\n",
    "\n",
    "        model.control_scales = (\n",
    "            [strength * (0.825 ** float(12 - i)) for i in range(13)]\n",
    "            if guess_mode\n",
    "            else ([strength] * 13)\n",
    "        )  # Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\n",
    "        samples, intermediates = ddim_sampler.sample(\n",
    "            ddim_steps,\n",
    "            num_samples,\n",
    "            shape,\n",
    "            cond,\n",
    "            verbose=False,\n",
    "            eta=eta,\n",
    "            unconditional_guidance_scale=scale,\n",
    "            unconditional_conditioning=un_cond,\n",
    "        )\n",
    "\n",
    "        if config.save_memory:\n",
    "            model.low_vram_shift(is_diffusing=False)\n",
    "\n",
    "        x_samples = model.decode_first_stage(samples)\n",
    "        x_samples = (\n",
    "            (einops.rearrange(x_samples, \"b c h w -> b h w c\") * 127.5 + 127.5)\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "            .clip(0, 255)\n",
    "            .astype(np.uint8)\n",
    "        )\n",
    "\n",
    "        results = [x_samples[i] for i in range(num_samples)]\n",
    "        output_stack = []\n",
    "        output_stack.append(img)\n",
    "        output_stack.append(detected_map)\n",
    "        output_stack.extend(results)\n",
    "        combined_image = np.hstack(output_stack)\n",
    "\n",
    "        # Set the text, font, size, and color\n",
    "        text = prompt\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        font_color = (255, 255, 255)  # White color\n",
    "        font_thickness = 2\n",
    "\n",
    "        # Measure the size of the text\n",
    "        text_size, _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n",
    "\n",
    "        # Create a new image with extra space at the bottom for the text\n",
    "        new_height = combined_image.shape[0] + text_size[1] * 2\n",
    "        new_image = np.zeros((new_height, combined_image.shape[1], 3), dtype=np.uint8)\n",
    "        new_image[: combined_image.shape[0], :, :] = combined_image\n",
    "\n",
    "        # Add the text to the new image\n",
    "        text_position = (10, int(combined_image.shape[0] + text_size[1] * 1.5))\n",
    "        cv2.putText(\n",
    "            new_image, text, text_position, font, font_scale, font_color, font_thickness\n",
    "        )\n",
    "\n",
    "        cv2.imwrite(output_dir, new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 12345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:17<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "input_image = \"./training/fusar_mix/target/Ship_C03S02N0004.png\"\n",
    "prompt = \"a Dredger ship in a top-down grayscale SAR image\"\n",
    "output_dir = \"./gen/str/Ship_C03S02N0004.png\"\n",
    "get_ims = process(input_image, prompt, output_dir, num_samples=4, strength=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Inference on CAM map](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_1_'></a>[0: Single Image Inference](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/workspace/dso/ControlSAR\")\n",
    "import importlib\n",
    "from inf_dir import inference_n\n",
    "from share import setup_config\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module 'xformers'. Proceeding without it.\n",
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loaded model config from [../models/cldm_v15.yaml]\n"
     ]
    }
   ],
   "source": [
    "setup_config()\n",
    "model = create_model(\"../models/cldm_v15.yaml\").cpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_2_'></a>[1: Single Image Inference](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state_dict from [../checkpoints/fusrs_v2_lc/fusrs_epoch=89.ckpt]\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(load_state_dict('./models/control_sd15_canny.pth', location='cuda'))\n",
    "# model.load_state_dict(load_state_dict(\"./checkpoints/fusrs_v2/256_cam_0/fusrs_epoch=5.ckpt\", location=\"cuda\"))\n",
    "# model.load_state_dict(load_state_dict(\"./checkpoints/fusrs_v2/256_cam_0/fusrs_epoch=13.ckpt\", location=\"cuda\"))\n",
    "model.load_state_dict(\n",
    "    load_state_dict(\n",
    "        # \"./checkpoints/fusar_v1/fusar_v1_ce-epoch=9.ckpt\", location=\"cuda:5\"\n",
    "        \"../checkpoints/fusrs_v2_lc/fusrs_epoch=89.ckpt\",\n",
    "        location=\"cuda:0\",\n",
    "    )\n",
    ")\n",
    "\n",
    "model = model.cuda()\n",
    "ddim_sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (6, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:06<00:00,  3.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# input_img = \"./training/fusrs_v2_256_cam/source/L274_8.png\"\n",
    "# prompt = \"a high-quality Dredger ship in a grayscale SAR radar satellite image, captured during a search and rescue operation\"\n",
    "# # prompt = \"a bird's-eye view of a Dredger ship navigating in the sea, captured in a SAR grayscale radar image\"\n",
    "# save_prefix = \"./gen/debug/epoch23_step20/L274_8\"\n",
    "\n",
    "input_img = \"../training/fusrs_v2_256_cam_v2/source/Ship_C04S02N0087.png\"\n",
    "# prompt = \"a high-quality Dredger ship in a grayscale SAR radar satellite image, captured during a search and rescue operation\"\n",
    "# prompt = \"Fishing ship in SAR image, fishing vessel, maneuverable hull, distinct activity pattern, marine resources, shorter and narrower profile, lower radar backscatter, unique operational behaviors, longer lingering time, rapid changes in direction, suspended fishing equipment, frequent turns, designated operation zones, efficient hull design, fishing-specific equipment, high-intensity scattering points, marine life tracking, compact ship construction, specialized storage compartments, handling fishing activities.\"\n",
    "prompt = \"Fishing ship in SAR image, smaller hull size, trawler, narrow profile, fishing gear, masts, booms, radar scattering points, reflection intensity, fishing nets, longlines, fishing pots, shorter length, sea activity patterns, storage sections, operational behavior, designated fishing areas, seasonal migration patterns, artisanal boats, commercial fishing boats.\"\n",
    "save_path = \"../gen/debug/230508/P2\"\n",
    "\n",
    "_ = inference_n(\n",
    "    input_img,\n",
    "    prompt,\n",
    "    model,\n",
    "    ddim_sampler,\n",
    "    save_path,\n",
    "    guess_mode=False,\n",
    "    num_samples=6,\n",
    "    image_resolution=256,\n",
    "    ddim_steps=20,\n",
    "    strength=1,\n",
    "    scale=9.0,\n",
    "    seed=0,\n",
    "    eta=0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc2_1_3_'></a>[1: Batch Inference](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlLDM: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Loaded model config from [./models/cldm_v15.yaml]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import inf_cam\n",
    "\n",
    "importlib.reload(inf_cam)\n",
    "from inf_cam import inference\n",
    "from share import setup_config\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from cldm.ddim_hacked import DDIMSampler\n",
    "\n",
    "setup_config()\n",
    "model = create_model(\"./models/cldm_v15.yaml\").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state_dict from [./checkpoints/fusrs_v2/256_cam_1/fusrs_epoch=10.ckpt]\n"
     ]
    }
   ],
   "source": [
    "# model.load_state_dict(load_state_dict('./models/control_sd15_canny.pth', location='cuda'))\n",
    "# model.load_state_dict(load_state_dict(\"./checkpoints/fusrs_v2/256_cam_0/fusrs_epoch=5.ckpt\", location=\"cuda\"))\n",
    "# model.load_state_dict(load_state_dict(\"./checkpoints/fusrs_v2/256_cam_0/fusrs_epoch=13.ckpt\", location=\"cuda\"))\n",
    "model.load_state_dict(\n",
    "    load_state_dict(\n",
    "        \"./checkpoints/fusrs_v2/256_cam_1/fusrs_epoch=10.ckpt\", location=\"cuda\"\n",
    "    )\n",
    ")\n",
    "\n",
    "model = model.cuda()\n",
    "ddim_sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L416_2.png a high-resolution grayscale SAR image depicting a Fishing ship sailing in rough seas ./gen/fusrs_v2_cam/epoch23_step20_eta0/P0_L416_2\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.65it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L416_2.png a bird's-eye view of a Fishing ship navigating in the sea, captured in a SAR grayscale radar image ./gen/fusrs_v2_cam/epoch23_step20_eta0/P1_L416_2\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.78it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L416_2.png a Fishing ship seen from a top-down grayscale satellite SAR radar image ./gen/fusrs_v2_cam/epoch23_step20_eta0/P2_L416_2\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:03<00:00,  5.03it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L416_2.png an aerial grayscale SAR image of a Fishing ship maneuvering on the sea ./gen/fusrs_v2_cam/epoch23_step20_eta0/P3_L416_2\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.71it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L416_2.png a Fishing ship in a grayscale SAR radar satellite image, captured during a search and rescue operation ./gen/fusrs_v2_cam/epoch23_step20_eta0/P4_L416_2\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.70it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L416_2.png a top-down view of a Fishing ship leaving a harbor in a high-resolution grayscale SAR satellite radar image ./gen/fusrs_v2_cam/epoch23_step20_eta0/P5_L416_2\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.63it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L416_2.png a Fishing ship conducting a maritime operation, as seen in a grayscale overhead SAR radar image ./gen/fusrs_v2_cam/epoch23_step20_eta0/P6_L416_2\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.73it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L416_2.png a bird's-eye view of a Fishing ship passing through an ice field, visible in a grayscale SAR image ./gen/fusrs_v2_cam/epoch23_step20_eta0/P7_L416_2\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.80it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L416_2.png a grayscale SAR satellite radar image capturing a Fishing ship near an offshore platform ./gen/fusrs_v2_cam/epoch23_step20_eta0/P8_L416_2\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.96it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L416_2.png a Fishing ship in a top-down grayscale SAR image, with visible deck equipment and structure ./gen/fusrs_v2_cam/epoch23_step20_eta0/P9_L416_2\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.68it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L443_4.png a high-resolution grayscale SAR image depicting a Cargo ship sailing in rough seas ./gen/fusrs_v2_cam/epoch23_step20_eta0/P10_L443_4\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.60it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L443_4.png a bird's-eye view of a Cargo ship navigating in the sea, captured in a SAR grayscale radar image ./gen/fusrs_v2_cam/epoch23_step20_eta0/P11_L443_4\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.92it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L443_4.png a Cargo ship seen from a top-down grayscale satellite SAR radar image ./gen/fusrs_v2_cam/epoch23_step20_eta0/P12_L443_4\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.72it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L443_4.png an aerial grayscale SAR image of a Cargo ship maneuvering on the sea ./gen/fusrs_v2_cam/epoch23_step20_eta0/P13_L443_4\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.73it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L443_4.png a Cargo ship in a grayscale SAR radar satellite image, captured during a search and rescue operation ./gen/fusrs_v2_cam/epoch23_step20_eta0/P14_L443_4\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.73it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L443_4.png a top-down view of a Cargo ship leaving a harbor in a high-resolution grayscale SAR satellite radar image ./gen/fusrs_v2_cam/epoch23_step20_eta0/P15_L443_4\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.75it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L443_4.png a Cargo ship conducting a maritime operation, as seen in a grayscale overhead SAR radar image ./gen/fusrs_v2_cam/epoch23_step20_eta0/P16_L443_4\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.86it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L443_4.png a bird's-eye view of a Cargo ship passing through an ice field, visible in a grayscale SAR image ./gen/fusrs_v2_cam/epoch23_step20_eta0/P17_L443_4\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.89it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L443_4.png a grayscale SAR satellite radar image capturing a Cargo ship near an offshore platform ./gen/fusrs_v2_cam/epoch23_step20_eta0/P18_L443_4\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.81it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L443_4.png a Cargo ship in a top-down grayscale SAR image, with visible deck equipment and structure ./gen/fusrs_v2_cam/epoch23_step20_eta0/P19_L443_4\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 20/20 [00:04<00:00,  4.88it/s]\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training/fusrs_v2_256_cam/source/./L274_8.png a high-resolution grayscale SAR image depicting a Dredger ship sailing in rough seas ./gen/fusrs_v2_cam/epoch23_step20_eta0/P20_L274_8\n",
      "Data shape for DDIM sampling is (1, 4, 32, 32), eta 0\n",
      "Running DDIM Sampling with 20 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  80%|████████  | 16/20 [00:03<00:00,  4.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m output_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./gen/fusrs_v2_cam/epoch23_step20_eta0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[39m# Call the generate_batch function\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m generate_batch(prompts_file, input_dir, output_dir, model, ddim_sampler)\n",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m, in \u001b[0;36mgenerate_batch\u001b[0;34m(prompts_file, input_dir, output_dir, model, ddim_sampler)\u001b[0m\n\u001b[1;32m     16\u001b[0m save_prefix \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(output_dir, save_img_prefix)\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(condition_path, prompt, save_prefix)\n\u001b[0;32m---> 18\u001b[0m _ \u001b[39m=\u001b[39m inference(\n\u001b[1;32m     19\u001b[0m     condition_path,\n\u001b[1;32m     20\u001b[0m     prompt,\n\u001b[1;32m     21\u001b[0m     model,\n\u001b[1;32m     22\u001b[0m     ddim_sampler,\n\u001b[1;32m     23\u001b[0m     save_prefix,\n\u001b[1;32m     24\u001b[0m     guess_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     25\u001b[0m     num_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m     image_resolution\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,\n\u001b[1;32m     27\u001b[0m     ddim_steps\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m     strength\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m     scale\u001b[39m=\u001b[39;49m\u001b[39m9.0\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m     seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     31\u001b[0m     eta\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m )\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/inf_cam.py:203\u001b[0m, in \u001b[0;36minference\u001b[0;34m(input_image, prompt, model, ddim_sampler, save_prefix, a_prompt, n_prompt, guess_mode, num_samples, image_resolution, ddim_steps, strength, scale, seed, eta)\u001b[0m\n\u001b[1;32m    196\u001b[0m     model\u001b[39m.\u001b[39mlow_vram_shift(is_diffusing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    198\u001b[0m model\u001b[39m.\u001b[39mcontrol_scales \u001b[39m=\u001b[39m (\n\u001b[1;32m    199\u001b[0m     [strength \u001b[39m*\u001b[39m (\u001b[39m0.825\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m12\u001b[39m \u001b[39m-\u001b[39m i)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m13\u001b[39m)]\n\u001b[1;32m    200\u001b[0m     \u001b[39mif\u001b[39;00m guess_mode\n\u001b[1;32m    201\u001b[0m     \u001b[39melse\u001b[39;00m ([strength] \u001b[39m*\u001b[39m \u001b[39m13\u001b[39m)\n\u001b[1;32m    202\u001b[0m )  \u001b[39m# Magic number. IDK why. Perhaps because 0.825**12<0.01 but 0.826**12>0.01\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m samples, intermediates \u001b[39m=\u001b[39m ddim_sampler\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m    204\u001b[0m     ddim_steps,\n\u001b[1;32m    205\u001b[0m     num_samples,\n\u001b[1;32m    206\u001b[0m     shape,\n\u001b[1;32m    207\u001b[0m     cond,\n\u001b[1;32m    208\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    209\u001b[0m     eta\u001b[39m=\u001b[39;49meta,\n\u001b[1;32m    210\u001b[0m     unconditional_guidance_scale\u001b[39m=\u001b[39;49mscale,\n\u001b[1;32m    211\u001b[0m     unconditional_conditioning\u001b[39m=\u001b[39;49mun_cond,\n\u001b[1;32m    212\u001b[0m )\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39msave_memory:\n\u001b[1;32m    215\u001b[0m     model\u001b[39m.\u001b[39mlow_vram_shift(is_diffusing\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/cldm/ddim_hacked.py:103\u001b[0m, in \u001b[0;36mDDIMSampler.sample\u001b[0;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, dynamic_threshold, ucg_schedule, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m size \u001b[39m=\u001b[39m (batch_size, C, H, W)\n\u001b[1;32m    101\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mData shape for DDIM sampling is \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m, eta \u001b[39m\u001b[39m{\u001b[39;00meta\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 103\u001b[0m samples, intermediates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mddim_sampling(conditioning, size,\n\u001b[1;32m    104\u001b[0m                                             callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    105\u001b[0m                                             img_callback\u001b[39m=\u001b[39;49mimg_callback,\n\u001b[1;32m    106\u001b[0m                                             quantize_denoised\u001b[39m=\u001b[39;49mquantize_x0,\n\u001b[1;32m    107\u001b[0m                                             mask\u001b[39m=\u001b[39;49mmask, x0\u001b[39m=\u001b[39;49mx0,\n\u001b[1;32m    108\u001b[0m                                             ddim_use_original_steps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    109\u001b[0m                                             noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout,\n\u001b[1;32m    110\u001b[0m                                             temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    111\u001b[0m                                             score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[1;32m    112\u001b[0m                                             corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[1;32m    113\u001b[0m                                             x_T\u001b[39m=\u001b[39;49mx_T,\n\u001b[1;32m    114\u001b[0m                                             log_every_t\u001b[39m=\u001b[39;49mlog_every_t,\n\u001b[1;32m    115\u001b[0m                                             unconditional_guidance_scale\u001b[39m=\u001b[39;49munconditional_guidance_scale,\n\u001b[1;32m    116\u001b[0m                                             unconditional_conditioning\u001b[39m=\u001b[39;49munconditional_conditioning,\n\u001b[1;32m    117\u001b[0m                                             dynamic_threshold\u001b[39m=\u001b[39;49mdynamic_threshold,\n\u001b[1;32m    118\u001b[0m                                             ucg_schedule\u001b[39m=\u001b[39;49mucg_schedule\n\u001b[1;32m    119\u001b[0m                                             )\n\u001b[1;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/cldm/ddim_hacked.py:163\u001b[0m, in \u001b[0;36mDDIMSampler.ddim_sampling\u001b[0;34m(self, cond, shape, x_T, ddim_use_original_steps, callback, timesteps, quantize_denoised, mask, x0, img_callback, log_every_t, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, dynamic_threshold, ucg_schedule)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(ucg_schedule) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(time_range)\n\u001b[1;32m    161\u001b[0m     unconditional_guidance_scale \u001b[39m=\u001b[39m ucg_schedule[i]\n\u001b[0;32m--> 163\u001b[0m outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample_ddim(img, cond, ts, index\u001b[39m=\u001b[39;49mindex, use_original_steps\u001b[39m=\u001b[39;49mddim_use_original_steps,\n\u001b[1;32m    164\u001b[0m                           quantize_denoised\u001b[39m=\u001b[39;49mquantize_denoised, temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    165\u001b[0m                           noise_dropout\u001b[39m=\u001b[39;49mnoise_dropout, score_corrector\u001b[39m=\u001b[39;49mscore_corrector,\n\u001b[1;32m    166\u001b[0m                           corrector_kwargs\u001b[39m=\u001b[39;49mcorrector_kwargs,\n\u001b[1;32m    167\u001b[0m                           unconditional_guidance_scale\u001b[39m=\u001b[39;49munconditional_guidance_scale,\n\u001b[1;32m    168\u001b[0m                           unconditional_conditioning\u001b[39m=\u001b[39;49munconditional_conditioning,\n\u001b[1;32m    169\u001b[0m                           dynamic_threshold\u001b[39m=\u001b[39;49mdynamic_threshold)\n\u001b[1;32m    170\u001b[0m img, pred_x0 \u001b[39m=\u001b[39m outs\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m callback: callback(i)\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/cldm/ddim_hacked.py:191\u001b[0m, in \u001b[0;36mDDIMSampler.p_sample_ddim\u001b[0;34m(self, x, c, t, index, repeat_noise, use_original_steps, quantize_denoised, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning, dynamic_threshold)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     model_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mapply_model(x, t, c)\n\u001b[0;32m--> 191\u001b[0m     model_uncond \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mapply_model(x, t, unconditional_conditioning)\n\u001b[1;32m    192\u001b[0m     model_output \u001b[39m=\u001b[39m model_uncond \u001b[39m+\u001b[39m unconditional_guidance_scale \u001b[39m*\u001b[39m (model_t \u001b[39m-\u001b[39m model_uncond)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameterization \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mv\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/cldm/cldm.py:408\u001b[0m, in \u001b[0;36mControlLDM.apply_model\u001b[0;34m(self, x_noisy, t, cond, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     control \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol_model(\n\u001b[1;32m    402\u001b[0m         x\u001b[39m=\u001b[39mx_noisy,\n\u001b[1;32m    403\u001b[0m         hint\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat(cond[\u001b[39m\"\u001b[39m\u001b[39mc_concat\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m1\u001b[39m),\n\u001b[1;32m    404\u001b[0m         timesteps\u001b[39m=\u001b[39mt,\n\u001b[1;32m    405\u001b[0m         context\u001b[39m=\u001b[39mcond_txt,\n\u001b[1;32m    406\u001b[0m     )\n\u001b[1;32m    407\u001b[0m     control \u001b[39m=\u001b[39m [c \u001b[39m*\u001b[39m scale \u001b[39mfor\u001b[39;00m c, scale \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(control, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol_scales)]\n\u001b[0;32m--> 408\u001b[0m     eps \u001b[39m=\u001b[39m diffusion_model(\n\u001b[1;32m    409\u001b[0m         x\u001b[39m=\u001b[39;49mx_noisy,\n\u001b[1;32m    410\u001b[0m         timesteps\u001b[39m=\u001b[39;49mt,\n\u001b[1;32m    411\u001b[0m         context\u001b[39m=\u001b[39;49mcond_txt,\n\u001b[1;32m    412\u001b[0m         control\u001b[39m=\u001b[39;49mcontrol,\n\u001b[1;32m    413\u001b[0m         only_mid_control\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monly_mid_control,\n\u001b[1;32m    414\u001b[0m     )\n\u001b[1;32m    416\u001b[0m \u001b[39mreturn\u001b[39;00m eps\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/cldm/cldm.py:58\u001b[0m, in \u001b[0;36mControlledUnetModel.forward\u001b[0;34m(self, x, timesteps, context, control, only_mid_control, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h, hs\u001b[39m.\u001b[39mpop() \u001b[39m+\u001b[39m control\u001b[39m.\u001b[39mpop()], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m     h \u001b[39m=\u001b[39m module(h, emb, context)\n\u001b[1;32m     60\u001b[0m h \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mtype(x\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m     61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(h)\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/ldm/modules/diffusionmodules/openaimodel.py:84\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb, context)\u001b[0m\n\u001b[1;32m     82\u001b[0m     x \u001b[39m=\u001b[39m layer(x, emb)\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, SpatialTransformer):\n\u001b[0;32m---> 84\u001b[0m     x \u001b[39m=\u001b[39m layer(x, context)\n\u001b[1;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/ldm/modules/attention.py:334\u001b[0m, in \u001b[0;36mSpatialTransformer.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    332\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_in(x)\n\u001b[1;32m    333\u001b[0m \u001b[39mfor\u001b[39;00m i, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_blocks):\n\u001b[0;32m--> 334\u001b[0m     x \u001b[39m=\u001b[39m block(x, context\u001b[39m=\u001b[39;49mcontext[i])\n\u001b[1;32m    335\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_linear:\n\u001b[1;32m    336\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_out(x)\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/ldm/modules/attention.py:269\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m checkpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward, (x, context), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint)\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/ldm/modules/diffusionmodules/util.py:114\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m flag:\n\u001b[1;32m    113\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(inputs) \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(params)\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(func, \u001b[39mlen\u001b[39;49m(inputs), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39minputs)\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/ldm/modules/diffusionmodules/util.py:129\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, length, *args)\u001b[0m\n\u001b[1;32m    125\u001b[0m ctx\u001b[39m.\u001b[39mgpu_autocast_kwargs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39menabled\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mis_autocast_enabled(),\n\u001b[1;32m    126\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mget_autocast_gpu_dtype(),\n\u001b[1;32m    127\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mcache_enabled\u001b[39m\u001b[39m\"\u001b[39m: torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()}\n\u001b[1;32m    128\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 129\u001b[0m     output_tensors \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39;49mrun_function(\u001b[39m*\u001b[39;49mctx\u001b[39m.\u001b[39;49minput_tensors)\n\u001b[1;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m output_tensors\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/ldm/modules/attention.py:274\u001b[0m, in \u001b[0;36mBasicTransformerBlock._forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    272\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x), context\u001b[39m=\u001b[39mcontext \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_self_attn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39m+\u001b[39m x\n\u001b[1;32m    273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x), context\u001b[39m=\u001b[39mcontext) \u001b[39m+\u001b[39m x\n\u001b[0;32m--> 274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mff(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm3(x)) \u001b[39m+\u001b[39m x\n\u001b[1;32m    275\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/ldm/modules/attention.py:76\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/control/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/dso/ControlSAR/ldm/modules/attention.py:56\u001b[0m, in \u001b[0;36mGEGLU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     55\u001b[0m     x, gate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(x)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39;49m F\u001b[39m.\u001b[39;49mgelu(gate)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def generate_batch(prompts_file, input_dir, output_dir, model, ddim_sampler):\n",
    "    with open(prompts_file, \"r\") as f:\n",
    "        # for idx, line in enumerate(f):\n",
    "        for idx in range(50):\n",
    "            line = f.readline()\n",
    "            data = json.loads(line)\n",
    "            condition_path = data[\"condition\"]\n",
    "            condition_path = os.path.join(input_dir, condition_path)\n",
    "            prompt = data[\"prompt\"]\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            save_img_prefix = (\n",
    "                f\"P{idx}_\" + os.path.splitext(os.path.basename(condition_path))[0]\n",
    "            )\n",
    "            save_prefix = os.path.join(output_dir, save_img_prefix)\n",
    "            print(condition_path, prompt, save_prefix)\n",
    "            _ = inference(\n",
    "                condition_path,\n",
    "                prompt,\n",
    "                model,\n",
    "                ddim_sampler,\n",
    "                save_prefix,\n",
    "                guess_mode=False,\n",
    "                num_samples=1,\n",
    "                image_resolution=256,\n",
    "                ddim_steps=20,\n",
    "                strength=1,\n",
    "                scale=9.0,\n",
    "                seed=0,\n",
    "                eta=0,\n",
    "            )\n",
    "\n",
    "\n",
    "# Set the paths\n",
    "# prompts_file = \"/workspace/dso/ControlSAR/gen/fusrs_v2_cam/cond+prompt.json\"\n",
    "prompts_file = \"/workspace/dso/ControlSAR/gen/fusrs_v2_cam/dredger.json\"\n",
    "input_dir = \"./training/fusrs_v2_256_cam/source\"\n",
    "output_dir = \"./gen/fusrs_v2_cam/epoch23_step20_eta0\"\n",
    "\n",
    "# Call the generate_batch function\n",
    "generate_batch(prompts_file, input_dir, output_dir, model, ddim_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
