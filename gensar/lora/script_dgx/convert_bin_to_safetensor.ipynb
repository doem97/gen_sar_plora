{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sd1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adapted from https://github.com/huggingface/diffusers/issues/2326 by https://github.com/ignacfetser\n",
    "\n",
    "The LoRA trained using Diffusers are saved in .bin or .pkl format, which must be converted to be used in Automatic1111 WebUI.\n",
    "\n",
    "This script converts .bin or .pkl files into .safetensors format, which can be used in WebUI.\n",
    "\n",
    "Put this file in the same folder of .bin or .pkl file and run `python convert-to-safetensors.py --file checkpoint_file`\n",
    "\n",
    "\"\"\"\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "\n",
    "def convert_bin(file_path):\n",
    "    ## use GPU or CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        checkpoint = torch.load(file_path, map_location=torch.device(\"cuda\"))\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        # if on CPU or want to have maximum precision on GPU, use default full-precision setting\n",
    "        checkpoint = torch.load(file_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "    print(f\"device is {device}\")\n",
    "\n",
    "    new_dict = dict()\n",
    "    for idx, key in enumerate(checkpoint):\n",
    "        new_key = re.sub(\"\\.processor\\.\", \"_\", key)\n",
    "        new_key = re.sub(\"mid_block\\.\", \"mid_block_\", new_key)\n",
    "        new_key = re.sub(\"_lora.up.\", \".lora_up.\", new_key)\n",
    "        new_key = re.sub(\"_lora.down.\", \".lora_down.\", new_key)\n",
    "        new_key = re.sub(\"\\.(\\d+)\\.\", \"_\\\\1_\", new_key)\n",
    "        new_key = re.sub(\"to_out\", \"to_out_0\", new_key)\n",
    "        new_key = \"lora_unet_\" + new_key\n",
    "\n",
    "        new_dict[new_key] = checkpoint[key]\n",
    "\n",
    "    file_name = os.path.splitext(file_path)[0]  \n",
    "    # get the file name without the extension\n",
    "    new_lora_name = file_name + \".safetensors\"\n",
    "    print(\"Saving \" + new_lora_name)\n",
    "    save_file(new_dict, new_lora_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n",
      "Saving /workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank1/checkpoint-20000/pytorch_model.safetensors\n",
      "device is cuda\n",
      "Saving /workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank2/checkpoint-20000/pytorch_model.safetensors\n",
      "device is cuda\n",
      "Saving /workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank4/checkpoint-20000/pytorch_model.safetensors\n",
      "device is cuda\n",
      "Saving /workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank8/checkpoint-20000/pytorch_model.safetensors\n",
      "device is cuda\n",
      "Saving /workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank16/checkpoint-20000/pytorch_model.safetensors\n",
      "device is cuda\n",
      "Saving /workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank32/checkpoint-20000/pytorch_model.safetensors\n",
      "device is cuda\n",
      "Saving /workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank64/checkpoint-20000/pytorch_model.safetensors\n",
      "device is cuda\n",
      "Saving /workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank128/checkpoint-20000/pytorch_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "# FPATH = \"/workspace/dso/GenSAR/LoRA/output/DOSRS_v1/shiprs_512_sd15_lr1e-04/checkpoint-2160/pytorch_model.bin\"\n",
    "# FPATH = \"/workspace/dso/GenSAR/LoRA/output/DOSRS_v1/512_sd15_lr1e-04/checkpoint-2700/pytorch_model.bin\"\n",
    "# FPATH = \"/workspace/dso/GenSAR/LoRA/output/sarlora256_lr1e-04/checkpoint-10000/pytorch_model.bin\"\n",
    "# FPATH = \"/workspace/dso/GenSAR/LoRA/output/DOSRS_v2/dosrsv2_512_sd15_lr1e-04/checkpoint-5400/pytorch_model.bin\"\n",
    "paths = [\"/workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank1/checkpoint-20000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank2/checkpoint-20000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank4/checkpoint-20000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank8/checkpoint-20000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank16/checkpoint-20000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank32/checkpoint-20000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank64/checkpoint-20000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/256/rank/256_fp32_s20000+100e_wp0_bs32_lr1e-03_rank128/checkpoint-20000\"]\n",
    "for path in paths:\n",
    "    binpath = path + \"/pytorch_model.bin\"\n",
    "    convert_bin(binpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n",
      "Saving /workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank1/checkpoint-11000/pytorch_model.safetensors\n",
      "device is cuda\n",
      "Saving /workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank2/checkpoint-11000/pytorch_model.safetensors\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank4/checkpoint-11000/pytorch_model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/dso/gensar/lora/script_dgx/convert_bin_to_safetensor.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7a637469616e2d5344222c2273657474696e6773223a7b22686f7374223a227373683a2f2f444758227d7d/workspace/dso/gensar/lora/script_dgx/convert_bin_to_safetensor.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m paths:\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7a637469616e2d5344222c2273657474696e6773223a7b22686f7374223a227373683a2f2f444758227d7d/workspace/dso/gensar/lora/script_dgx/convert_bin_to_safetensor.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     binpath \u001b[39m=\u001b[39m path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/pytorch_model.bin\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7a637469616e2d5344222c2273657474696e6773223a7b22686f7374223a227373683a2f2f444758227d7d/workspace/dso/gensar/lora/script_dgx/convert_bin_to_safetensor.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     convert_bin(binpath)\n",
      "\u001b[1;32m/workspace/dso/gensar/lora/script_dgx/convert_bin_to_safetensor.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7a637469616e2d5344222c2273657474696e6773223a7b22686f7374223a227373683a2f2f444758227d7d/workspace/dso/gensar/lora/script_dgx/convert_bin_to_safetensor.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7a637469616e2d5344222c2273657474696e6773223a7b22686f7374223a227373683a2f2f444758227d7d/workspace/dso/gensar/lora/script_dgx/convert_bin_to_safetensor.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7a637469616e2d5344222c2273657474696e6773223a7b22686f7374223a227373683a2f2f444758227d7d/workspace/dso/gensar/lora/script_dgx/convert_bin_to_safetensor.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(file_path, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7a637469616e2d5344222c2273657474696e6773223a7b22686f7374223a227373683a2f2f444758227d7d/workspace/dso/gensar/lora/script_dgx/convert_bin_to_safetensor.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7a637469616e2d5344222c2273657474696e6773223a7b22686f7374223a227373683a2f2f444758227d7d/workspace/dso/gensar/lora/script_dgx/convert_bin_to_safetensor.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/sd1/lib/python3.10/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/envs/sd1/lib/python3.10/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/envs/sd1/lib/python3.10/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank4/checkpoint-11000/pytorch_model.bin'"
     ]
    }
   ],
   "source": [
    "# FPATH = \"/workspace/dso/GenSAR/LoRA/output/DOSRS_v1/shiprs_512_sd15_lr1e-04/checkpoint-2160/pytorch_model.bin\"\n",
    "# FPATH = \"/workspace/dso/GenSAR/LoRA/output/DOSRS_v1/512_sd15_lr1e-04/checkpoint-2700/pytorch_model.bin\"\n",
    "# FPATH = \"/workspace/dso/GenSAR/LoRA/output/sarlora256_lr1e-04/checkpoint-10000/pytorch_model.bin\"\n",
    "# FPATH = \"/workspace/dso/GenSAR/LoRA/output/DOSRS_v2/dosrsv2_512_sd15_lr1e-04/checkpoint-5400/pytorch_model.bin\"\n",
    "paths = [\"/workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank1/checkpoint-11000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank2/checkpoint-11000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank4/checkpoint-11000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank8/checkpoint-11000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank16/checkpoint-11000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank32/checkpoint-11000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank64/checkpoint-11000\",\n",
    "         \"/workspace/dso/gensar/lora/output/sarlora/512/512_fp32_s11000_wp0_bs32_lr1e-03_rank128/checkpoint-11000\"]\n",
    "for path in paths:\n",
    "    binpath = path + \"/pytorch_model.bin\"\n",
    "    convert_bin(binpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I don't know what below is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargo: [3630, 99, 94, 32, 29, 0, 3, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "Other: [1520, 33, 64, 1, 21, 59, 5, 1, 3, 3, 0, 0, 0, 0, 0, 0]\n",
      "Fishing: [333, 383, 51, 45, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tanker: [10, 207, 26, 24, 4, 27, 13, 1, 3, 0, 0, 0, 0, 0, 0, 0]\n",
      "Dredger: [52, 129, 45, 5, 1, 5, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def split_into_subclasses(data, num_subclasses=16):\n",
    "    splits = {}\n",
    "    for category, value in data.items():\n",
    "        remaining = value\n",
    "        sub_values = []\n",
    "\n",
    "        for i in range(num_subclasses - 1):  # Subtract 1 to account for last split\n",
    "            split_val = random.randint(0, remaining)\n",
    "            sub_values.append(split_val)\n",
    "            remaining -= split_val\n",
    "\n",
    "        sub_values.append(remaining)  # the remaining value goes to the last subclass\n",
    "        splits[category] = sub_values\n",
    "\n",
    "    return splits\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = {\"Cargo\": 3890, \"Other\": 1710, \"Fishing\": 814, \"Tanker\": 315, \"Dredger\": 242}\n",
    "\n",
    "splits = split_into_subclasses(data)\n",
    "for category, sub_values in splits.items():\n",
    "    print(f\"{category}: {sub_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subclass 1: Cargo=241, Other=124, Fishing=43, Tanker=18, Dredger=24\n",
      "Subclass 2: Cargo=229, Other=107, Fishing=50, Tanker=24, Dredger=15\n",
      "Subclass 3: Cargo=246, Other=103, Fishing=54, Tanker=26, Dredger=19\n",
      "Subclass 4: Cargo=263, Other=110, Fishing=58, Tanker=16, Dredger=11\n",
      "Subclass 5: Cargo=276, Other=118, Fishing=45, Tanker=23, Dredger=11\n",
      "Subclass 6: Cargo=221, Other=101, Fishing=51, Tanker=21, Dredger=15\n",
      "Subclass 7: Cargo=244, Other=83, Fishing=53, Tanker=15, Dredger=9\n",
      "Subclass 8: Cargo=248, Other=111, Fishing=56, Tanker=15, Dredger=15\n",
      "Subclass 9: Cargo=240, Other=87, Fishing=51, Tanker=15, Dredger=12\n",
      "Subclass 10: Cargo=225, Other=104, Fishing=49, Tanker=24, Dredger=10\n",
      "Subclass 11: Cargo=266, Other=101, Fishing=53, Tanker=20, Dredger=24\n",
      "Subclass 12: Cargo=216, Other=116, Fishing=59, Tanker=17, Dredger=21\n",
      "Subclass 13: Cargo=237, Other=120, Fishing=52, Tanker=22, Dredger=16\n",
      "Subclass 14: Cargo=251, Other=110, Fishing=47, Tanker=20, Dredger=12\n",
      "Subclass 15: Cargo=248, Other=107, Fishing=46, Tanker=24, Dredger=17\n",
      "Subclass 16: Cargo=239, Other=108, Fishing=47, Tanker=15, Dredger=11\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def split_category(value, num_splits=16):\n",
    "    splits = [0] * num_splits\n",
    "    for _ in range(value):\n",
    "        splits[random.randint(0, num_splits - 1)] += 1\n",
    "    return splits\n",
    "\n",
    "\n",
    "data = {\"Cargo\": 3890, \"Other\": 1710, \"Fishing\": 814, \"Tanker\": 315, \"Dredger\": 242}\n",
    "\n",
    "subclasses = []\n",
    "\n",
    "# Split each category into 16 subclasses\n",
    "for category, value in data.items():\n",
    "    splits = split_category(value)\n",
    "    if not subclasses:\n",
    "        subclasses = [[val] for val in splits]\n",
    "    else:\n",
    "        for i, val in enumerate(splits):\n",
    "            subclasses[i].append(val)\n",
    "\n",
    "# Each element in subclasses now contains representation from each of the five main classes\n",
    "for idx, sc in enumerate(subclasses):\n",
    "    print(\n",
    "        f\"Subclass {idx + 1}: Cargo={sc[0]}, Other={sc[1]}, Fishing={sc[2]}, Tanker={sc[3]}, Dredger={sc[4]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
