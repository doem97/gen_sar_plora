{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [TOP1](#toc1_)    \n",
    "  - [Analysize `best.json`](#toc1_1_)    \n",
    "    - [Locate the `best.json` from target dir.](#toc1_1_1_)    \n",
    "    - [Calculate FPR for each category](#toc1_1_2_)    \n",
    "  - [Some other Metrics](#toc1_2_)    \n",
    "    - [Check TP, FP, TN, FN](#toc1_2_1_)    \n",
    "    - [Calculate accuracy for each category](#toc1_2_2_)    \n",
    "    - [macro, micro, weighted](#toc1_2_3_)    \n",
    "- [TOP_K](#toc2_)    \n",
    "  - [Calculate Voting Ensemble](#toc2_1_)    \n",
    "  - [Get top1 from K models](#toc2_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[TOP1](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Analysize `best.json`](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[Locate the `best.json` from target dir.](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# fusar\n",
    "# ann=\"../data/fusar/meta/test.txt\" # fusar\n",
    "# ann=\"../data/opensar/meta/test.txt\"; num_categories = 8 # opensar\n",
    "ann = \"../data/fusrs_v2/meta/test.txt\"\n",
    "num_categories = 5  # fusrs_v2\n",
    "\n",
    "# pred_dir=\"../outputs/res50_fusrs_v2_dreaug/res50_1x128_lr1e-1+200e+im21k_fusrs_v2\"\n",
    "# pred_dir=\"../outputs/res50_fusrs_v2_pretrain/res50_1x128_lr1e-1+200e_fusrs_v2\"\n",
    "pred_dir = \"../outputs/res50_fusrs_v2_dreaug/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam/best\"\n",
    "# pred_dir=\"../outputs/res50_fusrs_v2_pretrain/res50_1x128_lr1e-1+200e+im1k_fusrs_v2\"\n",
    "\n",
    "# Load predicted labels from pred.json\n",
    "# Find the only .json file starting with \"best_f1_score\" in pred_dir\n",
    "json_files = glob.glob(os.path.join(pred_dir, \"best_f1_score*.json\"))\n",
    "\n",
    "if len(json_files) == 1:\n",
    "    pred = json_files[0]\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Expected exactly one best_f1_score JSON file in pred_dir, but found {}.\".format(\n",
    "            len(json_files)\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(pred, \"r\") as f:\n",
    "    pred_data = json.load(f)\n",
    "    pred_labels = pred_data[\"pred_label\"]\n",
    "\n",
    "# Load ground truth labels from label.txt\n",
    "gt_labels = []\n",
    "with open(ann, \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        gt_labels.append(int(parts[-1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[Calculate FPR for each category](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0: Precision: 0.8984, Recall: 0.9624, F1-score: 0.9293\n",
      "Category 1: Precision: 0.8113, Recall: 0.7963, F1-score: 0.8037\n",
      "Category 2: Precision: 0.6870, Recall: 0.5590, F1-score: 0.6164\n",
      "Category 3: Precision: 0.7021, Recall: 0.5593, F1-score: 0.6226\n",
      "Category 4: Precision: 0.9302, Recall: 0.7692, F1-score: 0.8421\n",
      "Macro (unweighted average of) Precision: 0.8058, Recall: 0.7293, F1-score: 0.7629\n",
      "Weighted (weighted average of) Precision: 0.8486, Recall: 0.8556, F1-score: 0.8499\n",
      "Micro (micro average of) Precision: 0.8556, Recall: 0.8556, F1-score: 0.8556\n",
      "************** REFORMULATE **************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.898428</td>\n",
       "      <td>0.962435</td>\n",
       "      <td>0.929331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.803738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.687023</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.616438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.622642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.805826</td>\n",
       "      <td>0.729258</td>\n",
       "      <td>0.762851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.848603</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.849859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Precision    Recall        F1\n",
       "0   0.898428  0.962435  0.929331\n",
       "1   0.811321  0.796296  0.803738\n",
       "2   0.687023  0.559006  0.616438\n",
       "3   0.702128  0.559322  0.622642\n",
       "4   0.930233  0.769231  0.842105\n",
       "5   0.805826  0.729258  0.762851\n",
       "6   0.848603  0.855556  0.849859"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate per-cateogry FPR\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the per-category precision, recall, and F1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    gt_labels, pred_labels, average=None\n",
    ")\n",
    "\n",
    "# Print the results for each category\n",
    "for i in range(num_categories):\n",
    "    print(\n",
    "        \"Category {}: Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(\n",
    "            i, precision[i], recall[i], f1[i]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Calculate unweighted average (Macro) of FPR\n",
    "# Calculate macro F1-score\n",
    "macro_f1 = f1_score(gt_labels, pred_labels, average=\"macro\")\n",
    "macro_pre = precision_score(gt_labels, pred_labels, average=\"macro\")\n",
    "macro_rec = recall_score(gt_labels, pred_labels, average=\"macro\")\n",
    "# Print the results for weighted average of FPR\n",
    "print(\n",
    "    \"Macro (unweighted average of) Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(\n",
    "        macro_pre, macro_rec, macro_f1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate weighted average of FPR\n",
    "weighted_f1 = f1_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "weighted_pre = precision_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "weighted_rec = recall_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "# Print the results for weighted average of FPR\n",
    "print(\n",
    "    \"Weighted (weighted average of) Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(\n",
    "        weighted_pre, weighted_rec, weighted_f1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate all-category micro F1-score\n",
    "micro_f1 = f1_score(gt_labels, pred_labels, average=\"micro\")\n",
    "micro_pre = precision_score(gt_labels, pred_labels, average=\"micro\")\n",
    "micro_rec = recall_score(gt_labels, pred_labels, average=\"micro\")\n",
    "# Print the results for micro average of FPR\n",
    "print(\n",
    "    \"Micro (micro average of) Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(\n",
    "        micro_pre, micro_rec, micro_f1\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"************** REFORMULATE **************\")\n",
    "# Calculate unweighted average (Macro) of FPR\n",
    "# Calculate macro F1-score\n",
    "macro_f1 = f1_score(gt_labels, pred_labels, average=\"macro\")\n",
    "macro_pre = precision_score(gt_labels, pred_labels, average=\"macro\")\n",
    "macro_rec = recall_score(gt_labels, pred_labels, average=\"macro\")\n",
    "# Print the results for weighted average of FPR\n",
    "precision = np.append(precision, macro_pre)\n",
    "recall = np.append(recall, macro_rec)\n",
    "f1 = np.append(f1, macro_f1)\n",
    "\n",
    "# Calculate weighted average of FPR\n",
    "weighted_f1 = f1_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "weighted_pre = precision_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "weighted_rec = recall_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "# Print the results for weighted average of FPR\n",
    "precision = np.append(precision, weighted_pre)\n",
    "recall = np.append(recall, weighted_rec)\n",
    "f1 = np.append(f1, weighted_f1)\n",
    "\n",
    "data = {\"Precision\": precision, \"Recall\": recall, \"F1\": f1}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Display the DataFrame in the notebook\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Some other Metrics](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Check TP, FP, TN, FN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 0\n",
      "True positives: 743\n",
      "True negatives: 404\n",
      "False positives: 84\n",
      "False negatives: 29\n",
      "Category 1\n",
      "True positives: 172\n",
      "True negatives: 1004\n",
      "False positives: 40\n",
      "False negatives: 44\n",
      "Category 2\n",
      "True positives: 90\n",
      "True negatives: 1058\n",
      "False positives: 41\n",
      "False negatives: 71\n",
      "Category 3\n",
      "True positives: 33\n",
      "True negatives: 1187\n",
      "False positives: 14\n",
      "False negatives: 26\n",
      "Category 4\n",
      "True positives: 40\n",
      "True negatives: 1205\n",
      "False positives: 3\n",
      "False negatives: 12\n"
     ]
    }
   ],
   "source": [
    "def see_tp(gt_labels, pred_labels, category):\n",
    "    gt_labels = np.array(gt_labels)\n",
    "    pred_labels = np.array(pred_labels)\n",
    "\n",
    "    true_positives = np.sum((gt_labels == category) & (pred_labels == category))\n",
    "    false_positives = np.sum((gt_labels != category) & (pred_labels == category))\n",
    "    false_negatives = np.sum((gt_labels == category) & (pred_labels != category))\n",
    "    true_negatives = np.sum((gt_labels != category) & (pred_labels != category))\n",
    "\n",
    "    print(\"True positives: {}\".format(true_positives))\n",
    "    print(\"True negatives: {}\".format(true_negatives))\n",
    "    print(\"False positives: {}\".format(false_positives))\n",
    "    print(\"False negatives: {}\".format(false_negatives))\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Category {}\".format(i))\n",
    "    see_tp(gt_labels, pred_labels, i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Calculate accuracy for each category](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for category 0: 95.60%\n",
      "Accuracy for category 1: 84.72%\n",
      "Accuracy for category 2: 59.63%\n",
      "Accuracy for category 3: 52.54%\n",
      "Accuracy for category 4: 75.00%\n",
      "Average accuracy: 73.50%\n"
     ]
    }
   ],
   "source": [
    "correct_counts = [0] * num_categories\n",
    "total_counts = [0] * num_categories\n",
    "\n",
    "for pred_label, gt_label in zip(pred_labels, gt_labels):\n",
    "    total_counts[gt_label] += 1\n",
    "    if pred_label == gt_label:\n",
    "        correct_counts[gt_label] += 1\n",
    "\n",
    "# Calculate and print accuracy for each category\n",
    "accuracies = []\n",
    "for i in range(num_categories):\n",
    "    if total_counts[i] == 0:\n",
    "        print(\n",
    "            f\"Category {i}: got no GT samples, whereas {correct_counts[i]} samples are predicted.\"\n",
    "        )\n",
    "        continue\n",
    "    accuracy = correct_counts[i] / total_counts[i] * 100\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Accuracy for category {i}: {accuracy:.2f}%\")\n",
    "\n",
    "average_accuracy = sum(accuracies) / num_categories\n",
    "total_accuracy = sum(correct_counts) / sum(total_counts) * 100\n",
    "print(f\"Average accuracy: {average_accuracy:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[macro, micro, weighted](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro (unweighted average of) Precision: 0.5917, Recall: 0.6827, F1-score: 0.6102\n"
     ]
    }
   ],
   "source": [
    "# Calculate unweighted average (macro) FPR\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Assuming A is the ground truth labels and B is the predicted labels\n",
    "# You can replace these with your own variable names\n",
    "\n",
    "# Calculate macro F1-score\n",
    "macro_f1 = f1_score(gt_labels, pred_labels, average=\"macro\")\n",
    "macro_pre = precision_score(gt_labels, pred_labels, average=\"macro\")\n",
    "macro_rec = recall_score(gt_labels, pred_labels, average=\"macro\")\n",
    "\n",
    "# Print the results for weighted average of FPR\n",
    "print(\n",
    "    \"macro (unweighted average of) Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(\n",
    "        macro_pre, macro_rec, macro_f1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted (weighted average of) Precision: 0.7560, Recall: 0.7118, F1-score: 0.7253\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted average (weighted) FPR\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Assuming A is the ground truth labels and B is the predicted labels\n",
    "# You can replace these with your own variable names\n",
    "\n",
    "# Calculate weighted F1-score\n",
    "weighted_f1 = f1_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "weighted_pre = precision_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "weighted_rec = recall_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "\n",
    "# Print the results for weighted average of FPR\n",
    "print(\n",
    "    \"weighted (weighted average of) Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(\n",
    "        weighted_pre, weighted_rec, weighted_f1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro (micro average of) Precision: 0.7118, Recall: 0.7118, F1-score: 0.7118\n"
     ]
    }
   ],
   "source": [
    "# Calculate micro average (micro) FPR\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Assuming A is the ground truth labels and B is the predicted labels\n",
    "# You can replace these with your own variable names\n",
    "\n",
    "# Calculate micro F1-score\n",
    "micro_f1 = f1_score(gt_labels, pred_labels, average=\"micro\")\n",
    "micro_pre = precision_score(gt_labels, pred_labels, average=\"micro\")\n",
    "micro_rec = recall_score(gt_labels, pred_labels, average=\"micro\")\n",
    "\n",
    "# Print the results for micro average of FPR\n",
    "print(\n",
    "    \"micro (micro average of) Precision: {:.4f}, Recall: {:.4f}, F1-score: {:.4f}\".format(\n",
    "        micro_pre, micro_rec, micro_f1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[TOP_K](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Calculate Voting Ensemble](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# import glob\n",
    "import os\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def calc_results(\n",
    "    pred_labels: np.ndarray,\n",
    "    gt_labels: np.ndarray,\n",
    "    identifier: str,\n",
    "    log_dir: str = \"./log.txt\",\n",
    "    save: bool = True,\n",
    "):\n",
    "    \"\"\"calculate the voting ensemble results and log them to a file\n",
    "\n",
    "    Args:\n",
    "        pred_labels (np.ndarray): predicted labels\n",
    "        gt_labels (np.ndarray): ground truth labels\n",
    "        identifier (str): an identifier for the current experiment\n",
    "        log_dir (str, optional): directory to save the exp logs. Defaults to \"./log.txt\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the per-category precision, recall, and F1-score\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        gt_labels, pred_labels, average=None\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        with open(log_dir, \"a\") as log_f:\n",
    "            current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print(f\"\\n************** {current_time} **************\", file=log_f)\n",
    "            print(f\"Ensembled prediction for {identifier}\", file=log_f)\n",
    "            # Calculate unweighted average (Macro) of FPR\n",
    "            # Calculate macro F1-score\n",
    "            macro_f1 = f1_score(gt_labels, pred_labels, average=\"macro\")\n",
    "            macro_pre = precision_score(gt_labels, pred_labels, average=\"macro\")\n",
    "            macro_rec = recall_score(gt_labels, pred_labels, average=\"macro\")\n",
    "            # Print the results for weighted average of FPR\n",
    "            precision = np.append(precision, macro_pre)\n",
    "            recall = np.append(recall, macro_rec)\n",
    "            f1 = np.append(f1, macro_f1)\n",
    "\n",
    "            # Calculate weighted average of FPR\n",
    "            weighted_f1 = f1_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "            weighted_pre = precision_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "            weighted_rec = recall_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "            # Print the results for weighted average of FPR\n",
    "            precision = np.append(precision, weighted_pre)\n",
    "            recall = np.append(recall, weighted_rec)\n",
    "            f1 = np.append(f1, weighted_f1)\n",
    "\n",
    "            data = {\"Precision\": precision, \"Recall\": recall, \"F1\": f1}\n",
    "            df = pd.DataFrame(data)\n",
    "\n",
    "            # Display the DataFrame in the notebook\n",
    "            print(df, file=log_f)\n",
    "    else:\n",
    "        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"************** {current_time} **************\")\n",
    "        print(f\"Ensembled prediction for {identifier}\")\n",
    "        # Calculate unweighted average (Macro) of FPR\n",
    "        # Calculate macro F1-score\n",
    "        macro_f1 = f1_score(gt_labels, pred_labels, average=\"macro\")\n",
    "        macro_pre = precision_score(gt_labels, pred_labels, average=\"macro\")\n",
    "        macro_rec = recall_score(gt_labels, pred_labels, average=\"macro\")\n",
    "        # Print the results for weighted average of FPR\n",
    "        precision = np.append(precision, macro_pre)\n",
    "        recall = np.append(recall, macro_rec)\n",
    "        f1 = np.append(f1, macro_f1)\n",
    "\n",
    "        # Calculate weighted average of FPR\n",
    "        weighted_f1 = f1_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "        weighted_pre = precision_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "        weighted_rec = recall_score(gt_labels, pred_labels, average=\"weighted\")\n",
    "        # Print the results for weighted average of FPR\n",
    "        precision = np.append(precision, weighted_pre)\n",
    "        recall = np.append(recall, weighted_rec)\n",
    "        f1 = np.append(f1, weighted_f1)\n",
    "\n",
    "        data = {\"Precision\": precision, \"Recall\": recall, \"F1\": f1}\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Display the DataFrame in the notebook\n",
    "        print(df)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def main_vot(\n",
    "    ckpts_dir: str,\n",
    "    dataset: str = \"fusrs_v2\",\n",
    "    log: str = \"./log.txt\",\n",
    "    voting: int = 5,\n",
    "    save: bool = True,\n",
    "):\n",
    "    if dataset == \"fusrs_v2\":\n",
    "        ann = \"../data/fusrs_v2/meta/test.txt\"\n",
    "    else:\n",
    "        raise Exception(\"Dataset not supported\")\n",
    "\n",
    "        # Load ground truth labels from label.txt\n",
    "    gt_labels = []\n",
    "    with open(ann, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            gt_labels.append(int(parts[-1]))\n",
    "\n",
    "    f1_score_dict = {}\n",
    "    for ckpt in os.listdir(ckpts_dir):\n",
    "        if ckpt.endswith(\".json\") and re.match(r\"top\\d+_f1_score.*\\.json\", ckpt):\n",
    "            ckpt = os.path.join(ckpts_dir, ckpt)\n",
    "            with open(ckpt, \"r\") as f:\n",
    "                pred_data = json.load(f)\n",
    "                pred_labels = pred_data[\"pred_label\"]\n",
    "            macro_f1 = f1_score(gt_labels, pred_labels, average=\"macro\")\n",
    "            f1_score_dict[ckpt] = macro_f1\n",
    "\n",
    "    n = voting  # You can set your desired value for n\n",
    "    top_n_models = sorted(f1_score_dict, key=f1_score_dict.get, reverse=True)[:n]\n",
    "\n",
    "    # Load the predictions from the top n models\n",
    "    top_n_predictions = []\n",
    "    for model in top_n_models:\n",
    "        with open(model, \"r\") as f:\n",
    "            pred_data = json.load(f)\n",
    "            pred_labels = pred_data[\"pred_label\"]\n",
    "        top_n_predictions.append(pred_labels)\n",
    "\n",
    "    # Perform voting ensemble on the top n model predictions\n",
    "    ensemble_predictions = []\n",
    "    for preds in zip(*top_n_predictions):\n",
    "        ensemble_predictions.append(Counter(preds).most_common(1)[0][0])\n",
    "\n",
    "    # Calculate the F1 score for the ensemble predictions\n",
    "    calc_results(ensemble_predictions, gt_labels, ckpts_dir, log, save)\n",
    "    return gt_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** 2023-05-04 02:02:49 **************\n",
      "Ensembled prediction for ../outputs/res50_fusrs_v2_dreaug_portion/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+758\n",
      "   Precision    Recall        F1\n",
      "0   0.914920  0.961140  0.937461\n",
      "1   0.821101  0.828704  0.824885\n",
      "2   0.738806  0.614907  0.671186\n",
      "3   0.627451  0.542373  0.581818\n",
      "4   0.869565  0.769231  0.816327\n",
      "5   0.794369  0.743271  0.766335\n",
      "6   0.861001  0.866667  0.862486\n"
     ]
    }
   ],
   "source": [
    "LOG_PATH1 = \"../outputs/res50_fusrs_v2_dreaug_portion/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+758\"\n",
    "VOT = 5\n",
    "gt_labels, pred_labels = main_vot(\n",
    "    ckpts_dir=LOG_PATH1, dataset=\"fusrs_v2\", log=\"log.txt\", voting=VOT, save=False\n",
    ")\n",
    "# for i in range(5):\n",
    "#     print(\"Category {}\".format(i))\n",
    "#     see_tp(gt_labels, pred_labels, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** 2023-05-05 19:01:24 **************\n",
      "Ensembled prediction for ../outputs/res50_fusrs_v2_dreaug/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam/best2_0501\n",
      "   Precision    Recall        F1\n",
      "0   0.911548  0.961140  0.935687\n",
      "1   0.804444  0.837963  0.820862\n",
      "2   0.740157  0.583851  0.652778\n",
      "3   0.693878  0.576271  0.629630\n",
      "4   0.933333  0.807692  0.865979\n",
      "5   0.816672  0.753383  0.780987\n",
      "6   0.861994  0.867460  0.862645\n",
      "\n",
      "************** 2023-05-05 19:01:24 **************\n",
      "Ensembled prediction for ../outputs/res50_fusrs_v2_dreaug/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam/1659573870\n",
      "   Precision    Recall        F1\n",
      "0   0.913793  0.961140  0.936869\n",
      "1   0.816514  0.824074  0.820276\n",
      "2   0.753731  0.627329  0.684746\n",
      "3   0.679245  0.610169  0.642857\n",
      "4   0.883721  0.730769  0.800000\n",
      "5   0.809401  0.750696  0.776950\n",
      "6   0.864440  0.869048  0.865250\n",
      "\n",
      "************** 2023-05-05 19:01:24 **************\n",
      "Ensembled prediction for ../outputs/res50_fusrs_v2_dreaug/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam/42_1\n",
      "   Precision    Recall        F1\n",
      "0   0.901580  0.961140  0.930408\n",
      "1   0.827907  0.824074  0.825986\n",
      "2   0.750000  0.596273  0.664360\n",
      "3   0.688889  0.525424  0.596154\n",
      "4   0.795918  0.750000  0.772277\n",
      "5   0.792859  0.731382  0.757837\n",
      "6   0.855262  0.861905  0.856334\n"
     ]
    }
   ],
   "source": [
    "VOT = 5\n",
    "LOG_PATH2 = \"../outputs/res50_fusrs_v2_dreaug/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam/best2_0501\"\n",
    "_, _ = main_vot(\n",
    "    ckpts_dir=LOG_PATH2, dataset=\"fusrs_v2\", log=\"log.txt\", voting=VOT, save=False\n",
    ")\n",
    "LOG_PATH2 = \"../outputs/res50_fusrs_v2_dreaug/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam/1659573870\"\n",
    "_, _ = main_vot(\n",
    "    ckpts_dir=LOG_PATH2, dataset=\"fusrs_v2\", log=\"log.txt\", voting=VOT, save=False\n",
    ")\n",
    "LOG_PATH2 = \"../outputs/res50_fusrs_v2_dreaug/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam/42_1\"\n",
    "_, _ = main_vot(\n",
    "    ckpts_dir=LOG_PATH2, dataset=\"fusrs_v2\", log=\"log.txt\", voting=VOT, save=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_2_'></a>[Get top1 from K models](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20/top10_f1_score_epoch_162.json\n",
      "************** 2023-05-09 08:12:31 **************\n",
      "Ensembled prediction for ../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20\n",
      "   Precision    Recall        F1\n",
      "0   0.915842  0.958549  0.936709\n",
      "1   0.788793  0.847222  0.816964\n",
      "2   0.781513  0.577640  0.664286\n",
      "3   0.745098  0.644068  0.690909\n",
      "4   0.800000  0.769231  0.784314\n",
      "5   0.806249  0.759342  0.778636\n",
      "6   0.864122  0.868254  0.863573\n",
      "\n",
      "\n",
      "Model ../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20/top10_f1_score_epoch_168.json\n",
      "************** 2023-05-09 08:12:31 **************\n",
      "Ensembled prediction for ../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20\n",
      "   Precision    Recall        F1\n",
      "0   0.914920  0.961140  0.937461\n",
      "1   0.790598  0.856481  0.822222\n",
      "2   0.783333  0.583851  0.669039\n",
      "3   0.729167  0.593220  0.654206\n",
      "4   0.851064  0.769231  0.808081\n",
      "5   0.813816  0.752785  0.778202\n",
      "6   0.865460  0.869841  0.864804\n",
      "\n",
      "\n",
      "Model ../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20/top10_f1_score_epoch_133.json\n",
      "************** 2023-05-09 08:12:31 **************\n",
      "Ensembled prediction for ../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20\n",
      "   Precision    Recall        F1\n",
      "0   0.912990  0.965026  0.938287\n",
      "1   0.816143  0.842593  0.829157\n",
      "2   0.765625  0.608696  0.678201\n",
      "3   0.717391  0.559322  0.628571\n",
      "4   0.851064  0.769231  0.808081\n",
      "5   0.812643  0.748973  0.776459\n",
      "6   0.865843  0.871429  0.866470\n",
      "\n",
      "\n",
      "Model ../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20/top10_f1_score_epoch_199.json\n",
      "************** 2023-05-09 08:12:31 **************\n",
      "Ensembled prediction for ../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20\n",
      "   Precision    Recall        F1\n",
      "0   0.913687  0.959845  0.936197\n",
      "1   0.793103  0.851852  0.821429\n",
      "2   0.781513  0.577640  0.664286\n",
      "3   0.750000  0.610169  0.672897\n",
      "4   0.800000  0.769231  0.784314\n",
      "5   0.807661  0.753747  0.775824\n",
      "6   0.863770  0.868254  0.863181\n",
      "\n",
      "\n",
      "Model ../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20/top10_f1_score_epoch_195.json\n",
      "************** 2023-05-09 08:12:31 **************\n",
      "Ensembled prediction for ../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20\n",
      "   Precision    Recall        F1\n",
      "0   0.911330  0.958549  0.934343\n",
      "1   0.800000  0.833333  0.816327\n",
      "2   0.768595  0.577640  0.659574\n",
      "3   0.745098  0.644068  0.690909\n",
      "4   0.784314  0.769231  0.776699\n",
      "5   0.801867  0.756564  0.775571\n",
      "6   0.860981  0.865873  0.861098\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VOT = 5\n",
    "LOG_PATH = \"../outputs/res50_fusrs_v2_aug20p_run2/res50_1x128_lr1e-1+200e+im21k_fusrs_v2+ctrlcam_ep20\"\n",
    "DATASET = \"fusrs_v2\"\n",
    "\n",
    "if DATASET == \"fusrs_v2\":\n",
    "    ann = \"../data/fusrs_v2/meta/test.txt\"\n",
    "else:\n",
    "    raise Exception(\"DATASET not supported\")\n",
    "\n",
    "    # Load ground truth labels from label.txt\n",
    "gt_labels = []\n",
    "with open(ann, \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        gt_labels.append(int(parts[-1]))\n",
    "\n",
    "f1_score_dict = {}\n",
    "for ckpt in os.listdir(LOG_PATH):\n",
    "    if ckpt.endswith(\".json\") and re.match(r\"top\\d+_f1_score.*\\.json\", ckpt):\n",
    "        ckpt = os.path.join(LOG_PATH, ckpt)\n",
    "        with open(ckpt, \"r\") as f:\n",
    "            pred_data = json.load(f)\n",
    "            pred_labels = pred_data[\"pred_label\"]\n",
    "        macro_f1 = f1_score(gt_labels, pred_labels, average=\"macro\")\n",
    "        f1_score_dict[ckpt] = macro_f1\n",
    "\n",
    "n = VOT  # You can set your desired value for n\n",
    "top_n_models = sorted(f1_score_dict, key=f1_score_dict.get, reverse=True)[:n]\n",
    "\n",
    "# Load the predictions from the top n models\n",
    "top_n_predictions = []\n",
    "for model in top_n_models:\n",
    "    with open(model, \"r\") as f:\n",
    "        pred_data = json.load(f)\n",
    "        pred_labels = pred_data[\"pred_label\"]\n",
    "    top_n_predictions.append(pred_labels)\n",
    "\n",
    "# Calculate the F1 score for the ensemble predictions\n",
    "for idx, pred_labels in enumerate(top_n_predictions):\n",
    "    print(\"Model {}\".format(top_n_models[idx]))\n",
    "    calc_results(pred_labels, gt_labels, LOG_PATH, \"_\", False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysize single ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "VOT = 5\n",
    "PRED = \"../outputs/res50_fusrs_v2_pretrain/res50_1x128_lr1e-1+200e+im21k_fusrs_v2/best_f1_score_epoch_158_train+val.json\"\n",
    "OUT_PATH = \"../outputs/cam_vis/res50_fusrs_v2_pretrain/res50_1x128_lr1e-1+200e+im21k_fusrs_v2/correct_predictions.txt\"\n",
    "DATASET = \"fusrs_v2\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if DATASET == \"fusrs_v2\":\n",
    "    # ann = \"../data/fusrs_v2/meta/test.txt\"\n",
    "    ann = \"../data/fusrs_v2/meta/train+val.txt\"  # WARN: temporary this\n",
    "else:\n",
    "    raise Exception(\"DATASET not supported\")\n",
    "\n",
    "    # Load ground truth labels from label.txt\n",
    "gt_labels = []\n",
    "with open(ann, \"r\") as f:\n",
    "    ann_lines = f.readlines()\n",
    "\n",
    "for line in ann_lines:\n",
    "    parts = line.strip().split()\n",
    "    gt_labels.append(int(parts[-1]))\n",
    "\n",
    "f1_score_dict = {}\n",
    "with open(PRED, \"r\") as f:\n",
    "    pred_data = json.load(f)\n",
    "    pred_labels = pred_data[\"pred_label\"]\n",
    "\n",
    "parent_dir = Path(OUT_PATH).parent\n",
    "os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "# Open destination file for writing\n",
    "with open(OUT_PATH, \"w\") as output_file:\n",
    "    # Iterate through ground truth and predicted labels, along with the corresponding lines from the annotation file\n",
    "    for gt_label, pred_label, ann_line in zip(gt_labels, pred_labels, ann_lines):\n",
    "        # Check if the prediction is correct\n",
    "        if gt_label == pred_label:\n",
    "            # Write the correct prediction (image path and ground truth label) to the output file\n",
    "            output_file.write(ann_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmcls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85aeabe64b0b9a3d37264c7d6760ff7d19a6a43c00eacb351da283e1b1336f21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
